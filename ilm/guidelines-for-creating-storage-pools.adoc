---
permalink: ilm/guidelines-for-creating-storage-pools.html
sidebar: sidebar
keywords: storage pool guidelines
summary: 'Configure and use storage pools to protect against data loss by distributing data across multiple sites. Replicated copies and erasure-coded copies require different storage pool configurations.'
---
= Guidelines for creating storage pools
:icons: font
:imagesdir: ../media/

[.lead]
Configure and use storage pools to protect against data loss by distributing data across multiple sites. Replicated copies and erasure-coded copies require different storage pool configurations.

See link:using-multiple-storage-pools-for-cross-site-replication.html[Examples of enabling site-loss protection using replication and erasure coding].

== Guidelines for all storage pools

* Keep storage pool configurations as simple as possible. Don't create more storage pools than necessary.
* Create storage pools with as many nodes as possible. Each storage pool should contain two or more nodes. A storage pool with insufficient nodes can cause ILM backlogs if a node becomes unavailable.
* Avoid creating or using storage pools that overlap (contain one or more of the same nodes). If storage pools overlap, more than one copy of object data might be saved on the same node.
* In general, don't use the All Storage Nodes storage pool (StorageGRID 11.6 and earlier) or the All Sites site. These items are automatically updated to include any new sites you add in an expansion, which might not be the behavior you want.

== Guidelines for storage pools used for replicated copies

* For site-loss protection using link:what-replication-is.html[replication], specify one or more site-specific storage pools in the link:create-ilm-rule-define-placements.html[placement instructions for each ILM rule].
+
One storage pool is automatically created for each site during StorageGRID installation.
+
Using a storage pool for each site ensures that replicated object copies are placed exactly where you expect (for example, one copy of every object at each site for site-loss protection).
* If you add a site in an expansion, create a new storage pool that contains only the new site. Then, link:working-with-ilm-rules-and-ilm-policies.html#edit-an-ilm-rule[update ILM rules] to control which objects are stored on the new site.
* If the number of copies is less than the number of storage pools, the system distributes the copies to balance disk usage among the pools.
* If the storage pools overlap (contain the same Storage Nodes), all copies of the object might be saved at only one site. You must ensure that the selected storage pools don't contain the same Storage Nodes.

== Guidelines for storage pools used for erasure-coded copies

* For site-loss protection using link:what-erasure-coding-is.html[erasure coding], create storage pools that consist of at least three sites. If a storage pool includes only two sites, you can't use that storage pool for erasure coding. No erasure-coding schemes are available for a storage pool that has two sites.
* The number of Storage Nodes and sites contained in the storage pool determine which link:what-erasure-coding-schemes-are.html[erasure-coding schemes] are available.
* If possible, a storage pool should include more than the minimum number of Storage Nodes required for the erasure-coding scheme you select. For example, if you use a 6+3 erasure-coding scheme, you must have at least nine Storage Nodes. However, having at least one additional Storage Node per site is recommended.
* Distribute Storage Nodes across sites as evenly as possible. For example, to support a 6+3 erasure-coding scheme, configure a storage pool that includes at least three Storage Nodes at three sites.
* If you have high throughput requirements, using a storage pool that includes multiple sites is not recommended if the network latency between sites is greater than 100 ms. As latency increases, the rate at which StorageGRID can create, place, and retrieve object fragments decreases sharply due to the decrease in TCP network throughput.
+
The decrease in throughput affects the maximum achievable rates of object ingest and retrieval (when Balanced or Strict are selected as the ingest behavior) or could lead to ILM queue backlogs (when Dual commit is selected as the ingest behavior). See link:what-ilm-rule-is.html#ilm-rule-ingest-behavior[ILM rule ingest behavior].
+
NOTE: If your grid includes only one site, you are prevented from using the All Storage Nodes storage pool (StorageGRID 11.6 and earlier) or the All Sites default site in an erasure coding profile. This behavior prevents the profile from becoming invalid if a second site is added.
* You can't use Archive Nodes for erasure-coded data.

== Guidelines for storage pools used for archived copies

[CAUTION]
====
Support for Archive Nodes (for both archiving to the cloud using the S3 API and archiving to tape using TSM middleware) is deprecated and will be removed in a future release. Moving objects from an Archive Node to an external archival storage system has been replaced by ILM Cloud Storage Pools, which offer more functionality.

See link:../admin/migrating-objects-from-cloud-tiering-s3-to-cloud-storage-pool.html[Migrate objects to a Cloud Storage Pool].

In addition, you should remove Archive Nodes from the active ILM policy in StorageGRID 11.7 or earlier. Removing object data stored on Archive Nodes will simplify future upgrades. See link:../ilm/working-with-ilm-rules-and-ilm-policies.html[Working with ILM rules and ILM policies].
====

* You can't create a storage pool that includes both Storage Nodes and Archive Nodes. Archived copies require a storage pool that only includes Archive Nodes.
* When using a storage pool that includes Archive Nodes, you should also maintain at least one replicated or erasure-coded copy on a storage pool that includes Storage Nodes.
* If the global S3 Object Lock setting is enabled and you are creating a compliant ILM rule, you can't use a storage pool that includes Archive Nodes. See the instructions for managing objects with S3 Object Lock.
* If an Archive Node's Target Type is Cloud Tiering - Simple Storage Service (S3), the Archive Node must be in its own storage pool.
